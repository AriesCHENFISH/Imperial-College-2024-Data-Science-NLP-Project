{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representation in Biomedical Domain\n",
    "\n",
    "Before you start, please make sure you have read this notebook. You are encouraged to follow the recommendations but you are also free to develop your own solution from scratch. \n",
    "\n",
    "## Marking Scheme\n",
    "\n",
    "- Biomedical imaging project: 40%\n",
    "    - 20%: accuracy of the final model on the test set\n",
    "    - 20%: rationale of model design and final report\n",
    "- Natural language processing project: 40%\n",
    "    - 30%: completeness of the project\n",
    "    - 10%: final report\n",
    "- Presentation skills and team work: 20%\n",
    "\n",
    "\n",
    "This project forms 40\\% of the total score for summer/winter school. The marking scheme of each part of this project is provided below with a cap of 100\\%.\n",
    "\n",
    "You are allowed to use open source libraries as long as the libraries are properly cited in the code and final report. The usage of third-party code without proper reference will be treated as plagiarism, which will not be tolerated.\n",
    "\n",
    "You are encouraged to develop the algorithms by yourselves (without using third-party code as much as possible). We will factor such effort into the marking process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Prerequisites \n",
    "\n",
    "Recommended environment\n",
    "\n",
    "- Python 3.7 or newer\n",
    "- Free disk space: 100GB\n",
    "\n",
    "Download the data\n",
    "\n",
    "```sh\n",
    "# navigate to the data folder\n",
    "cd data\n",
    "\n",
    "# download the data file\n",
    "# which is also available at https://www.semanticscholar.org/cord19/download\n",
    "wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2021-07-26/document_parses.tar.gz\n",
    "\n",
    "# decompress the file which may take several minutes\n",
    "tar -xf document_parses.tar.gz\n",
    "\n",
    "# which creates a folder named document_parses\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (20%): Parse the Data\n",
    "\n",
    "The JSON files are located in two sub-folders in `document_parses`. You will need to scan all JSON files and extract text (i.e. `string`) from relevant fields (e.g. body text, abstract, titles).\n",
    "\n",
    "You are encouraged to extract full article text from body text if possible. If the hardware resource is limited, you can extract from abstract or titles as alternatives. \n",
    "\n",
    "Note: The number of JSON files is around 425k so it may take more than 10 minutes to parse all documents.\n",
    "\n",
    "For more information about the dataset: https://www.semanticscholar.org/cord19/download\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- A list of text (`string`) extracted from JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: add your solution\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "# Set folder path\n",
    "folder_path = 'D:/pythonProject1/extraction/document_parses/pdf_json'\n",
    "\n",
    "# Set output file path\n",
    "output_file_path = 'D:/NLP_python/files/pdf_json_1.txt'\n",
    "\n",
    "# Traverse all JSON files in the folder\n",
    "abstract_texts = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "            try:\n",
    "                data = json.load(json_file)\n",
    "\n",
    "                # Handle the case where the 'abstract' field might be a list\n",
    "                abstract_list = data.get('abstract', [])\n",
    "\n",
    "                if isinstance(abstract_list, list):\n",
    "                    for abstract_item in abstract_list:\n",
    "                        abstract_text = abstract_item.get('text', '')\n",
    "                        abstract_texts.append(abstract_text)\n",
    "                else:\n",
    "                    abstract_text = abstract_list.get('text', '')\n",
    "                    abstract_texts.append(abstract_text)\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON in file: {file_path}\")\n",
    "\n",
    "# Write the extracted 'text' field to a new txt file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    for abstract_text in abstract_texts:\n",
    "        output_file.write(f\"{abstract_text}\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Segmentation completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (30%): Tokenization\n",
    "\n",
    "Traverse the extracted text and segment the text into words (or tokens).\n",
    "\n",
    "The following tracks can be developed in independentely. You are encouraged to divide the workload to each team member.\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- Tokenizer(s) that is able to tokenize any input text.\n",
    "\n",
    "Note: Because of the computation complexity of tokenizers, it may take hours/days to process all documents. Which tokenizer is more efficient? Any idea to speedup?\n",
    "\n",
    "### Track 2.1 (10%): Use split()\n",
    "\n",
    "Use the standard `split()` by Python.\n",
    "\n",
    "### Track 2.2 (10%): Use NLTK or SciSpaCy\n",
    "\n",
    "NLTK tokenizer: https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "SciSpaCy: https://github.com/allenai/scispacy\n",
    "\n",
    "Note: You may need to install NLTK and SpaCy so please refer to their websites for installation instructions.\n",
    "\n",
    "### Track 2.3 (10%): Use Byte-Pair Encoding (BPE)\n",
    "\n",
    "Byte-Pair Encoding (BPE): https://huggingface.co/transformers/tokenizer_summary.html\n",
    "\n",
    "Note: You may need to install Huggingface's transformers so please refer to its website for installation instructions.\n",
    "\n",
    "### Track 2.4 (Bonus +5%): Build new Byte-Pair Encoding (BPE)\n",
    "\n",
    "This track may be dependent on track 2.3.\n",
    "\n",
    "The above pre-built tokenization methods may not be suitable for biomedical domain as the words/tokens (e.g. diseases, sympotoms, chemicals, medications, phenotypes, genotypes etc.) can be very different from the words/tokens commonly used in daily life. Can you build and train a new BPE model for biomedical domain in particular?\n",
    "\n",
    "### Open Question (Optional):\n",
    "\n",
    "- What are the pros and cons of the above tokenizers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: add your solution\n",
    "# 2.1\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def segment_text_into_words(file_path):\n",
    "    \"\"\"\n",
    "    Segment the text into words (or tokens) using Python's split function.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the input text file.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of segmented words.\n",
    "    \"\"\"\n",
    "    segmented_words = []\n",
    "\n",
    "    # Define regular expression to remove punctuation and digits\n",
    "    regex = re.compile(r'[^\\w\\s]|\\d')\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        total_lines = sum(1 for line in file)\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file, total=total_lines, desc=\"Processing\"):\n",
    "            # Remove punctuation and digits, then split the line into words (or tokens)\n",
    "            line = regex.sub('', line)\n",
    "            words = line.split()\n",
    "            segmented_words.extend(words)\n",
    "\n",
    "    return segmented_words\n",
    "\n",
    "# Input file path\n",
    "input_file_path = '1output.txt'\n",
    "output_file_path = '2.1output.txt'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Segment the text from input file into words\n",
    "segmented_words = segment_text_into_words(input_file_path)\n",
    "\n",
    "# Save the segmented words to output file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    for word in segmented_words:\n",
    "        output_file.write(word + '\\n')\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Segmentation completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "import string\n",
    "import time\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def read_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stopwords_list = set(file.read().splitlines())\n",
    "    return stopwords_list\n",
    "\n",
    "def nltk_tokenizer(text):\n",
    "    # Use RegexpTokenizer to exclude punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def process_text(file_content, custom_stop_words, output_file):\n",
    "    filtered_words_list = []\n",
    "\n",
    "    for sentence in sent_tokenize(file_content):\n",
    "        tokens = nltk_tokenizer(sentence)\n",
    "        filtered_words = [word.lower() for word in tokens if word.lower() not in custom_stop_words]\n",
    "        filtered_words_list.extend(filtered_words)\n",
    "\n",
    "    # Write filtered words in larger chunks\n",
    "    output_file.write(\" \".join(filtered_words_list) + \"\\n\")\n",
    "\n",
    "# Example file paths\n",
    "stopwords_file_path = r\"D:/pythonProject1/extraction/stopwords-en.txt\"\n",
    "input_file_path = r\"files/pdf_json_1.txt\"\n",
    "output_file_path = r\"files/pdf_json_2.2.txt\"\n",
    "\n",
    "custom_stop_words = read_stopwords(stopwords_file_path)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file, \\\n",
    "     open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "\n",
    "    for line in input_file:\n",
    "        process_text(line, custom_stop_words, output_file)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Segmentation completed in {elapsed_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def bpe_tokenizer(text, vocab_size=30000):\n",
    "    \"\"\"\n",
    "    Tokenize the input text using Byte-Pair Encoding (BPE).\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text to be tokenized.\n",
    "    vocab_size (int): Vocabulary size for BPE.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tokens.\n",
    "    \"\"\"\n",
    "    # Initialize the BPE tokenizer\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "    # Train the BPE tokenizer\n",
    "    tokenizer.train_from_iterator([text], vocab_size=vocab_size)\n",
    "\n",
    "    # Tokenize using the trained BPE tokenizer\n",
    "    encoded = tokenizer.encode(text)\n",
    "    tokens = encoded.tokens\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Read the txt file containing abstracts\n",
    "input_file_path = r\"C:\\Users\\think\\PycharmProjects\\pythonProject2\\abstracts1.txt\"\n",
    "output_file_path = r\"C:\\Users\\think\\PycharmProjects\\pythonProject2\\2.3output.txt\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "    # Read the entire file content\n",
    "    file_content = input_file.read()\n",
    "\n",
    "# Remove all symbols, keeping only words\n",
    "file_content = re.sub(r'[^\\w\\s]', '', file_content)\n",
    "\n",
    "# Tokenize using the BPE tokenizer\n",
    "tokens = bpe_tokenizer(file_content)\n",
    "\n",
    "# Use NLTK for part-of-speech tagging, keeping only nouns\n",
    "tagged_tokens = nltk.pos_tag(word_tokenize(file_content))\n",
    "noun_tokens = [token[0] for token in tagged_tokens if token[1] in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    # Write nouns to the file, separated by spaces\n",
    "    output_file.write(\" \".join(noun_tokens))\n",
    "\n",
    "# Tokenization results have been saved to a new file\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Tokenization completed in {elapsed_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def initialize_vocab(data):\n",
    "    vocab = set()\n",
    "    for word in data:\n",
    "        vocab.update(list(word))\n",
    "    return vocab\n",
    "\n",
    "def get_stats(data):\n",
    "    stats = {}\n",
    "    for word in data:\n",
    "        symbols = list(word)\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i + 1])\n",
    "            stats[pair] = stats.get(pair, 0) + 1\n",
    "    return stats\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    new_vocab = set()\n",
    "    bigram = ''.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    for word in vocab:\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab.add(new_word)\n",
    "    return new_vocab\n",
    "\n",
    "def learn_bpe(data, num_merges, progress_bar=True):\n",
    "    vocab = initialize_vocab(data)\n",
    "\n",
    "    start_time = time.time()\n",
    "    progress_bar = tqdm(range(num_merges)) if progress_bar else range(num_merges)\n",
    "    for _ in progress_bar:\n",
    "        stats = get_stats(data)\n",
    "        best_pair = max(stats, key=stats.get)\n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'BPE model trained in {elapsed_time:.2f} seconds')\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read().splitlines()\n",
    "    return data\n",
    "\n",
    "import re\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def filter_nouns_and_remove_non_english(text):\n",
    "    filtered_data = []\n",
    "    for sentence in text:\n",
    "        # Remove non-English characters\n",
    "        english_text = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "\n",
    "        # Tokenize and POS tag\n",
    "        tokens = word_tokenize(english_text)\n",
    "        tagged = pos_tag(tokens)\n",
    "\n",
    "        # Keep nouns and convert to lowercase\n",
    "        nouns = [word.lower() for word, pos in tagged if pos.startswith('N')]\n",
    "\n",
    "        # Reassemble the sentence\n",
    "        filtered_sentence = ' '.join(nouns)\n",
    "        filtered_data.append(filtered_sentence)\n",
    "    return filtered_data\n",
    "\n",
    "# Read text file\n",
    "biomedical_data = read_text_file(r\"pdf_json_1.txt\")\n",
    "\n",
    "# Keep only nouns and remove symbols, numbers, and special characters\n",
    "filtered_data = filter_nouns_and_remove_non_english(biomedical_data)\n",
    "\n",
    "biomedical_bpe_vocab = learn_bpe(filtered_data, num_merges=100, progress_bar=True)\n",
    "\n",
    "# Save biomedical BPE vocabulary to file\n",
    "with open('biomedical_bpe_vocab.txt', 'w', encoding='utf-8') as file:\n",
    "    for token in biomedical_bpe_vocab:\n",
    "        file.write(token + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (30%): Build Word Representations\n",
    "\n",
    "Build word representations for each extracted word. If the hardware resource is limited, you may limit the vocabulary size up to 10k words/tokens (or even smaller) and the dimension of representations up to 256.\n",
    "\n",
    "The following tracks can be developed independently. You are encouraged to divide the workload to each team member.\n",
    "\n",
    "### Track 3.1 (15%): Use N-gram Language Modeling\n",
    "\n",
    "N-gram Language Modeling is to predict a target word by using `n` words from previous context. Specifically,\n",
    "\n",
    "$P(w_i | w_{i-1}, w_{i-2}, ..., w_{i-n+1})$\n",
    "\n",
    "For example, given a sentence, `\"the main symptoms of COVID-19 are fever and cough\"`, if $n=7$, we use previous context `[\"the\", \"main\", \"symptoms\", \"of\", \"COVID-19\", \"are\"]` to predict the next word `\"fever\"`.\n",
    "\n",
    "More to read: https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A fixed vector for each word/token.\n",
    "\n",
    "### Track 3.2 (15%): Use Skip-gram with Negative Sampling\n",
    "\n",
    "In skip-gram, we use a central word to predict its context. Specifically,\n",
    "\n",
    "$P(w_{c-m}, ... w_{c-1}, w_{c+1}, ..., w_{c+m} | w_c)$\n",
    "\n",
    "As the learning objective of skip-gram is computational inefficient (summation of entire vocabulary $|V|$), negative sampling is commonly applied to accelerate the training.\n",
    "\n",
    "In negative sampling, we randomly select one word from the context as a positive sample, and randomly select $K$ words from the vocabulary as negative samples. As a result, the learning objective is updated to\n",
    "\n",
    "$L = -\\log\\sigma(u^T_{t} v_c) - \\sum_{k=1}^K\\log\\sigma(-u^T_k v_c)$, where $u_t$ is the vector embedding of positive sample from context, $u_k$ are the vector embeddings of negative samples, $v_c$ is the vector embedding of the central word, $\\sigma$ refers to the sigmoid function.\n",
    "\n",
    "More to read http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf section 4.3 and 4.4\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A fixed vector for each word/token.\n",
    "\n",
    "### Track 3.3 (Bonus +5%): Use Contextualised Word Representation by Masked Language Model (MLM)\n",
    "\n",
    "BERT introduces a new language model for pre-training named Masked Language Model (MLM). The advantage of MLM is that the word representations by MLM will be contextualised.\n",
    "\n",
    "For example, \"stick\" may have different meanings in different context. By N-gram language modeling and word2vec (skip-gram, CBOW), the word representation of \"stick\" is fixed regardless of its context. However, MLM will learn the representation of \"stick\" dynamatically based on context. In other words, \"stick\" will have different representations in different context by MLM.\n",
    "\n",
    "More to read: http://jalammar.github.io/illustrated-bert/ and https://arxiv.org/pdf/1810.04805.pdf\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- An algorithm that is able to generate contextualised representation in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: add your solution\n",
    "# 3.1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# A class defining the NGram language model\n",
    "class NGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, context_size):\n",
    "        super(NGramModel, self).__init__()\n",
    "        # Create an nn.Embedding layer to map words to word vectors\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        # Create a linear layer to map input context vectors to a hidden layer\n",
    "        self.linear1 = nn.Linear(context_size * embed_size, 128)\n",
    "        # Create another linear layer to map the output of the hidden layer to the vocabulary size\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    # Forward method to define the forward pass of the model\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "# Build vocabulary from given text\n",
    "def build_vocab(text):\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    vocab = set(words)\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return vocab, word2idx, idx2word\n",
    "\n",
    "# Build N-Gram sequences from given text\n",
    "def build_ngrams(text, n):\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    ngrams = [tuple(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
    "    return ngrams\n",
    "\n",
    "# Train the N-Gram model\n",
    "def train_ngram_model(text, n, vocab_size, word2idx, num_epoch, lr, device, weight_decay):\n",
    "    ngrams = build_ngrams(text, n)\n",
    "    losses = []\n",
    "\n",
    "    model = NGramModel(vocab_size, 100, n)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    ngrams_idxs = []\n",
    "    for context_words in ngrams:\n",
    "        context_idxs = torch.tensor([word2idx[word] for word in context_words], dtype=torch.long).to(device)\n",
    "        ngrams_idxs.append(context_idxs)\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        total_loss = 0\n",
    "        for i in tqdm(range(len(ngrams)), desc=f'Epoch {epoch + 1}/{num_epoch}', unit='batch'):\n",
    "            context_words = ngrams[i]\n",
    "            context_idxs = ngrams_idxs[i].to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            log_probs = model(context_idxs)\n",
    "            target = torch.tensor([word2idx[context_words[-1]]], dtype=torch.long).to(device)\n",
    "            loss = loss_function(log_probs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        torch.save(model.state_dict(), f'ngram_model_ep{epoch}.pth')\n",
    "        print(f'Epoch {epoch + 1}/{num_epoch}, Loss: {total_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Determine the current device (GPU or CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_epoch = 10\n",
    "    lr = 1e-2\n",
    "    n = 3\n",
    "    weight_decay = 0\n",
    "\n",
    "    # Read text file\n",
    "    file_path = \"pdf_json_2.2.txt\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Build vocabulary and training data\n",
    "    vocab, word2idx, idx2word = build_vocab(text)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Train N-Gram model\n",
    "    model = train_ngram_model(text, n, vocab_size, word2idx, num_epoch, lr, device, weight_decay)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'ngram_model.pth')\n",
    "\n",
    "    # Get trained word vectors\n",
    "    word_vectors = model.embeddings.weight.data.numpy()\n",
    "\n",
    "    # Write words and corresponding vector representations to a text file\n",
    "    with open(\"3.1new_word_vectors.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        for word, idx in word2idx.items():\n",
    "            vector = \",\".join(str(num) for num in word_vectors[idx])\n",
    "            file.write(f\"{word},{vector}\\n\")\n",
    "\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# vocab_size represents the size of the vocabulary, embed_size represents the dimension of word vectors,\n",
    "# context_size represents the size of the context (i.e., the length of the considered N-Gram)\n",
    "class NGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, context_size):\n",
    "        super(NGramModel, self).__init__()\n",
    "        # Create an nn.Embedding layer to map words to word vectors\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        # Create a linear layer to map input context vectors to a hidden layer\n",
    "        self.linear1 = nn.Linear(context_size * embed_size, 128)\n",
    "        # Create another linear layer to map the output of the hidden layer to the vocabulary size\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    # inputs is a tensor representing the input context\n",
    "    def forward(self, inputs):\n",
    "        # Map the input context inputs through the embedding layer self.embeddings,\n",
    "        # converting each word index to the corresponding word vector.\n",
    "        # Then, use the view method to reshape the result into a tensor with shape (1, -1),\n",
    "        # where -1 represents the automatically calculated dimension, maintaining the first dimension as 1.\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        # Pass the mapped word vectors embeds to a linear layer self.linear1,\n",
    "        # applying the ReLU activation function. This linear layer maps the word vectors to a hidden layer.\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        # Pass the output of the hidden layer to another linear layer self.linear2,\n",
    "        # which maps the output of the hidden layer to the size of the vocabulary,\n",
    "        # obtaining the original output of the model.\n",
    "        out = self.linear2(out)\n",
    "        # Apply LogSoftmax operation to the original output of the model,\n",
    "        # computing the logarithmic probabilities for each word.\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        # Return the calculated log probabilities as the final output of the model.\n",
    "        return log_probs\n",
    "\n",
    "# A function to build a vocabulary from the given text and create mappings from words to indices and indices to words.\n",
    "# It takes a string parameter text representing the input text content.\n",
    "def build_vocab(text):\n",
    "    # Use the regular expression r'\\w+' to find all words in the text,\n",
    "    # where \\w+ matches one or more consecutive letters or digits.\n",
    "    # Convert the text to lowercase for uniform processing.\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    # Convert the list of found words to a set, removing duplicate words, and obtain the vocabulary vocab.\n",
    "    vocab = set(words)\n",
    "    # Create a dictionary word2idx, mapping each word in the vocabulary to a unique index.\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    # Create another dictionary idx2word, mapping indices back to the original words.\n",
    "    # This dictionary is useful for looking up words based on indices.\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return vocab, word2idx, idx2word\n",
    "\n",
    "# A function to build N-Gram sequences from the given text.\n",
    "# It takes two parameters: a string text representing the input text content and an integer n representing the length of N-Grams.\n",
    "def build_ngrams(text, n):\n",
    "    # Use the regular expression r'\\w+' to find all words in the text,\n",
    "    # where \\w+ matches one or more consecutive letters or digits.\n",
    "    # Convert the text to lowercase for uniform processing.\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    # Use a list comprehension to generate all N-Grams, forming a list.\n",
    "    ngrams = [tuple(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
    "    # Return these N-Gram sequences.\n",
    "    return ngrams\n",
    "\n",
    "# A function to train the N-Gram model with a progress bar.\n",
    "# It takes four parameters: a string text representing the input text content,\n",
    "# an integer n representing the length of N-Grams, an integer vocab_size representing the size of the vocabulary,\n",
    "# and a dictionary word2idx representing the mapping from words to indices.\n",
    "def train_ngram_model_with_progress(text, n, vocab_size, word2idx):\n",
    "    ngrams = build_ngrams(text, n)\n",
    "    losses = []\n",
    "    model = NGramModel(vocab_size, 100, n)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        total_loss = 0\n",
    "        # Use tqdm to add a progress bar\n",
    "        for context_words in tqdm(ngrams, desc=f'Epoch {epoch + 1}/{10}'):\n",
    "            context_idxs = torch.tensor([word2idx[word] for word in context_words], dtype=torch.long)\n",
    "\n",
    "            model.zero_grad()\n",
    "            log_probs = model(context_idxs)\n",
    "            target = torch.tensor([word2idx[context_words[-1]]], dtype=torch.long)\n",
    "            loss = loss_function(log_probs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        losses.append(total_loss)\n",
    "        print(f'Epoch {epoch + 1}/{10}, Loss: {total_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Read text file\n",
    "file_path = \"pdf_json_2.2.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Build vocabulary and training data\n",
    "vocab, word2idx, idx2word = build_vocab(text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Train N-Gram model with progress bar\n",
    "n = 3\n",
    "model = train_ngram_model_with_progress(text, n, vocab_size, word2idx)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'ngram_model.pth')\n",
    "\n",
    "# Get trained word vectors\n",
    "word_vectors = model.embeddings.weight.data.numpy()\n",
    "\n",
    "# Write words and corresponding vector representations to a text file\n",
    "with open(\"3.1new_word_vectors.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for word, idx in word2idx.items():\n",
    "        vector = \",\".join(str(num) for num in word_vectors[idx])\n",
    "        file.write(f\"{word},{vector}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Read the text file\n",
    "with open('pdf_json_2.2.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Load the BertTokenizer\n",
    "model_name = \"D:/pythonProject1/extraction/Bert/biobert_v1.1_pubmed/model.ckpt-1000000.index\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "\n",
    "# Process the text using BertTokenizer\n",
    "tokenized_text = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "\n",
    "# Print the processed results\n",
    "print(tokenized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (20%): Explore the Word Representations\n",
    "\n",
    "The following tracks can be finished independently. You are encouraged to divide workload to each team member.\n",
    "\n",
    "### Track 4.1 (5%): Visualise the word representations by t-SNE\n",
    "\n",
    "t-SNE is an algorithm to reduce dimentionality and commonly used to visualise high-dimension vectors. Use t-SNE to visualise the word representations. You may visualise up to 1000 words as t-SNE is highly computationally complex.\n",
    "\n",
    "More about t-SNE: https://lvdmaaten.github.io/tsne/\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- A diagram by t-SNE based on representations of up to 1000 words.\n",
    "\n",
    "### Track 4.2 (5%): Visualise the Word Representations of Biomedical Entities by t-SNE\n",
    "\n",
    "Instead of visualising the word representations of the entire vocabulary (or 1000 words that are selected at random), visualise the word representations of words which are biomedical entities. For example, fever, cough, diabetes etc. Based on the category of those biomedical entities, can you assign different colours to the entities and see if the entities from the same category can be clustered by t-SNE? For example, sinusitis and cough are both respirtory diseases so they should be assigned with the same colour and ideally their representations should be close to each other by t-SNE. Another example, Alzheimer and headache are neuralogical diseases which should be assigned by another colour.\n",
    "\n",
    "Examples of biomedial ontology: https://www.ebi.ac.uk/ols/ontologies/hp and https://en.wikipedia.org/wiki/International_Classification_of_Diseases\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- A diagram with colours by t-SNE based on representations of biomedical entities.\n",
    "\n",
    "### Track 4.3 (5%): Co-occurrence\n",
    "\n",
    "- What are the biomedical entities which frequently co-occur with COVID-19 (or coronavirus)?\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A sorted list of biomedical entities and description on how the entities are selected and sorted.\n",
    "\n",
    "### Track 4.4 (5%): Semantic Similarity\n",
    "\n",
    "- What are the biomedical entities which have closest semantic similarity COVID-19 (or coronavirus) based on word representations?\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A sorted list of biomedical entities and description on how the entities are selected and sorted.\n",
    "\n",
    "### Open Question (Optional): What else can you discover?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: add your solution\n",
    "# 4.1\n",
    "# TODO: add your solution\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# From txt file, load word representations where each line consists of a word and its corresponding vector representation\n",
    "with open('files/pdf_json_3.2true.txt', 'r', encoding='utf-8') as file:\n",
    "    word_vectors = [line.strip().split() for line in file]\n",
    "\n",
    "# Extract words and vector representations\n",
    "words, vectors = zip(*[(line[0], list(map(float, line[1:]))) for line in word_vectors])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "vectors = np.array(vectors)\n",
    "\n",
    "# Choose t-SNE parameters\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Use t-SNE for dimensionality reduction\n",
    "word_tsne = np.zeros((len(words), 2))  # Initialize array to store t-SNE results\n",
    "for i in tqdm(range(0, len(words), 1000), desc=\"t-SNE Progress\"):\n",
    "    end_idx = min(i + 1000, len(words))\n",
    "    word_tsne[i:end_idx] = tsne.fit_transform(vectors[i:end_idx])\n",
    "\n",
    "with open('files/tsne_results_1.txt', 'w', encoding='utf-8') as tsne_file:\n",
    "    for i, word in enumerate(words):\n",
    "        tsne_file.write(f\"{word} {word_tsne[i, 0]} {word_tsne[i, 1]}\\n\")\n",
    "\n",
    "# Visualize the results without labels, with smaller points, and semi-transparent colors\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(word_tsne[:, 0], word_tsne[:, 1], alpha=0.5, s=10)  # Adjust alpha for transparency and s for point size\n",
    "\n",
    "plt.title('t-SNE Visualization of Word Representations')\n",
    "plt.show()\n",
    "\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2\n",
    "def read_vectors_from_file(file_path):\n",
    "    word_vectors = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vector = [float(val) for val in parts[1:]]\n",
    "            word_vectors[word] = vector\n",
    "    return word_vectors\n",
    "\n",
    "# Read vectors from the text file\n",
    "file_path = 'pdf_json_3.2true.txt'  # Replace with your file path\n",
    "word_vectors = read_vectors_from_file(file_path)\n",
    "\n",
    "import requests\n",
    "\n",
    "def get_biomedical_entities_from_ols(ontology_id):\n",
    "    ols_api_url = f'https://www.ebi.ac.uk/ols/api/ontologies/{ontology_id}/terms?size=2000'\n",
    "    response = requests.get(ols_api_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        terms = response.json().get('_embedded', {}).get('terms', [])\n",
    "        biomedical_entities = [term['label'] for term in terms]\n",
    "        return biomedical_entities\n",
    "    else:\n",
    "        print(f\"Error accessing OLS API. Status Code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Get a list of biomedical entities\n",
    "biomedical_entities_from_ols = get_biomedical_entities_from_ols('hp')\n",
    "\n",
    "from difflib import get_close_matches\n",
    "\n",
    "def match_biomedical_entities_with_vectors(word_vectors, biomedical_entities, threshold=0.7):\n",
    "    matched_biomedical_entities = {}\n",
    "\n",
    "    for entity in biomedical_entities:\n",
    "        closest_matches = get_close_matches(entity, word_vectors.keys(), n=1, cutoff=threshold)\n",
    "        if closest_matches:\n",
    "            matched_biomedical_entities[entity] = {\n",
    "                'word': closest_matches[0],\n",
    "                'vector': word_vectors[closest_matches[0]]\n",
    "            }\n",
    "\n",
    "    return matched_biomedical_entities\n",
    "\n",
    "# Match words in the text with biomedical entities\n",
    "matched_biomedical_entities = match_biomedical_entities_with_vectors(word_vectors, biomedical_entities_from_ols)\n",
    "\n",
    "output_file_path = 'matched_biomedical_entities.txt'\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    for entity, match_info in matched_biomedical_entities.items():\n",
    "        output_file.write(f\"{match_info['word']}\\n\")\n",
    "\n",
    "print(f\"Matching results written to: {output_file_path}\")\n",
    "\n",
    "# K-means part\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# File paths\n",
    "file_path = 'matched_biomedical_entities.txt'\n",
    "\n",
    "# Read words into an array\n",
    "word_array = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        word = line.strip().lower()  # Convert to lowercase\n",
    "        word_array.append(word)\n",
    "\n",
    "def read_vectors_from_file(file_path):\n",
    "    word_vectors = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0].lower()  # Convert to lowercase\n",
    "            vector = [float(val) for val in parts[1:]]\n",
    "            word_vectors[word] = vector\n",
    "    print(f\"Read {len(word_vectors)} vectors from file.\")\n",
    "    return word_vectors\n",
    "\n",
    "# Read file containing word vectors\n",
    "word_vectors_file_path = 'tsne_results.txt'\n",
    "word_vectors = read_vectors_from_file(word_vectors_file_path)\n",
    "\n",
    "# Get vectors corresponding to matched words\n",
    "vectors_for_clustering = np.array([word_vectors[word] for word in word_array])\n",
    "\n",
    "# Use t-SNE to map high-dimensional vectors to 2D space\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=0)\n",
    "vectors_2d = tsne.fit_transform(vectors_for_clustering)\n",
    "\n",
    "# Use the elbow method to determine the optimal number of clusters\n",
    "def calculate_wcss(data, max_clusters=10):\n",
    "    wcss = []\n",
    "    for i in range(1, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        kmeans.fit(data)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    return wcss\n",
    "\n",
    "max_clusters_to_try = 10\n",
    "wcss_values = calculate_wcss(vectors_for_clustering, max_clusters=max_clusters_to_try)\n",
    "\n",
    "# Plot the elbow method graph\n",
    "plt.plot(range(1, max_clusters_to_try + 1), wcss_values, marker='o')\n",
    "plt.title('Elbow Method for Optimal Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.show()\n",
    "\n",
    "# Choose the optimal number of clusters based on the elbow method\n",
    "optimal_clusters = 4\n",
    "\n",
    "# Perform clustering using K-means algorithm\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "cluster_labels = kmeans.fit_predict(vectors_for_clustering)\n",
    "\n",
    "# Plot a scatter plot of the clustering results\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(optimal_clusters):\n",
    "    cluster_points = vectors_2d[cluster_labels == i]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i}')\n",
    "\n",
    "# Add labels\n",
    "for word, x, y in zip(word_array, vectors_2d[:, 0], vectors_2d[:, 1]):\n",
    "    plt.annotate(word, (x, y), textcoords=\"offset points\", xytext=(0, 5), ha='center', fontsize=8)\n",
    "\n",
    "plt.title('t-SNE Visualization of Clusters')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Write clustering results to file\n",
    "output_cluster_file_path = 'cluster_results_new.txt'\n",
    "with open(output_cluster_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    for word, cluster_label in zip(word_array, cluster_labels):\n",
    "        output_file.write(f\"Word: {word}, Cluster: {cluster_label}\\n\")\n",
    "\n",
    "print(f\"Cluster results written to: {output_cluster_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3\n",
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read segmented text data from a text file\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        texts = file.readlines()\n",
    "    return texts\n",
    "\n",
    "# Build co-occurrence matrix with progress bar\n",
    "def build_cooccurrence_matrix(texts):\n",
    "    vocab = set()\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        vocab.update(words)\n",
    "\n",
    "    vocab = list(vocab)\n",
    "    vocab_indices = {word: index for index, word in enumerate(vocab)}\n",
    "\n",
    "    cooccurrence_matrix = dok_matrix((len(vocab), len(vocab)), dtype=np.float64)\n",
    "\n",
    "    for text in tqdm(texts, desc=\"Building Co-occurrence Matrix\"):\n",
    "        words = text.split()\n",
    "        word_indices = [vocab_indices[word] for word in words if word in vocab]\n",
    "        for i in range(len(word_indices)):\n",
    "            for j in range(i + 1, len(word_indices)):\n",
    "                cooccurrence_matrix[word_indices[i], word_indices[j]] += 1\n",
    "                cooccurrence_matrix[word_indices[j], word_indices[i]] += 1\n",
    "\n",
    "    return cooccurrence_matrix, vocab_indices\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Read text data\n",
    "    file_path = 'D:\\pythonProject1\\extraction\\pdf_json_2_100percent.txt'\n",
    "    texts = read_text_file(file_path)\n",
    "    print(\"1\")\n",
    "    # Build co-occurrence matrix\n",
    "    cooccurrence_matrix, vocab_indices = build_cooccurrence_matrix(texts)\n",
    "    print(\"2\")\n",
    "    # Optionally: Normalize the co-occurrence matrix\n",
    "    row_sums = np.array(cooccurrence_matrix.sum(axis=1)).flatten()\n",
    "    row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "    cooccurrence_matrix_normalized = cooccurrence_matrix.tocsr() / row_sums[:, np.newaxis]\n",
    "\n",
    "\n",
    "    # In addition to the existing code\n",
    "\n",
    "    # Find words co-occurring with \"COVID19\" based on cooccurrence_matrix and vocab_indices\n",
    "    covid_related_entities = ['COVID19']\n",
    "    cooccurrence_counts = Counter()\n",
    "\n",
    "    for entity in covid_related_entities:\n",
    "        if entity in vocab_indices:\n",
    "            index = vocab_indices[entity]\n",
    "            cooccurrence_counts[entity] = cooccurrence_matrix[index, :].sum()\n",
    "\n",
    "    # Sort by co-occurrence frequency\n",
    "    sorted_entities = sorted(cooccurrence_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_vectors_from_txt(file_path):\n",
    "    # Load embedding vectors from a txt file\n",
    "    vectors = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(' ')\n",
    "            entity = parts[0]\n",
    "            vector = np.array([float(val) for val in parts[1:]])\n",
    "            vectors[entity] = vector\n",
    "    return vectors\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Calculate cosine similarity\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def generate_wordcloud(sorted_entities):\n",
    "    wordcloud_text = {entity: float(score) for entity, score in sorted_entities}\n",
    "\n",
    "    # Create WordCloud object\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(wordcloud_text)\n",
    "\n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    vectors_file_path = 'D:\\pythonProject1\\extraction\\pdf_json_3.2true.txt'  # Replace with your file path\n",
    "    output_file_path = 'output_similarity.txt'\n",
    "\n",
    "    # Load embedding vectors with replacements\n",
    "    loaded_vectors = load_vectors_from_txt(vectors_file_path)\n",
    "\n",
    "    # Extract the vector for corona\n",
    "    corona_vector = loaded_vectors.get('corona')\n",
    "\n",
    "    if corona_vector is not None:\n",
    "        print(\"Successfully extracted the vector for corona\")\n",
    "    else:\n",
    "        print(\"Vector for corona not found\")\n",
    "\n",
    "    # Calculate similarity scores with progress bar\n",
    "    similarity_scores = {}\n",
    "    for entity, entity_vector in tqdm(loaded_vectors.items(), desc=\"Calculating Similarity Scores\"):\n",
    "        similarity_scores[entity] = cosine_similarity(corona_vector, entity_vector)\n",
    "\n",
    "    # Sort the results\n",
    "    sorted_entities = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Output results to a new txt file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for entity, similarity_score in sorted_entities:\n",
    "            output_file.write(f\"{entity}: {similarity_score}\\n\")\n",
    "\n",
    "    # Generate and display the word cloud\n",
    "    generate_wordcloud(sorted_entities)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 (Bonus +10%): Open Challenge: Mining Biomedical Knowledge\n",
    "\n",
    "A fundamental task in clinical/biomedical natural language processing is to extract intelligence from biomedical text corpus automatically and efficiently. More specifically, the intelligence may include biomedical entities mentioned in text, relations between biomedical entities, clinical features of patients, progression of diseases, all of which can be used to predict, understand and improve patients' outcomes. \n",
    "\n",
    "This open challenge is to build a biomedical knowledge graph based on the CORD-19 dataset and mine useful information from it. We recommend the following steps but you are also encouraged to develop your solution from scratch.\n",
    "\n",
    "### Extract Biomedical Entities from Text\n",
    "\n",
    "Extract biomedical entities (such as fever, cough, headache, lung cancer, heart attack) from text. Note that:\n",
    "\n",
    "- The biomedical entities may consist of multiple words. For example, heart attack, multiple myeloma etc.\n",
    "- The biomedical entities may be written in synoynms. For example, low blood pressure for hypotension.\n",
    "- The biomedical entities may be written in different forms. For example, smoking, smokes, smoked.\n",
    "\n",
    "### Extract Relations between Biomedical Entities\n",
    "\n",
    "Extract relations between biomedical entities based on their appearance in text. You may define a relation between biomedical entities by one or more of the following criteria:\n",
    "\n",
    "- The biomedical entities frequentely co-occuer together.\n",
    "- The biomedical entities have similar word representations.\n",
    "- The biomedical entities have clear relations based on textual narratives. For example, \"The most common symptoms for COVID-19 are fever and cough\" so we know there are relations between \"COVID-19\", \"fever\" and \"cough\".\n",
    "\n",
    "### Build a Biomedical Knowledge Graph of COVID-19\n",
    "\n",
    "Build a knoweledge graph based on the results from track 5.1 and 5.2 and visualise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: add your solution\n",
    "CREATE (n1:MedicalResearchFields {name:'Epidemiology'})\n",
    "CREATE (n2:MedicalResearchFields {name:'Pulmonology'})\n",
    "CREATE (n3:MedicalResearchFields {name:'Immunology'})\n",
    "CREATE (n4:symptom{name:'Dyspneay'})\n",
    "CREATE (n5:symptom {name:'Cough'})\n",
    "CREATE (n6:symptom {name:'Fever'})\n",
    "CREATE (n7:Treatment{name:'Anti-inflammatory Drugs'})\n",
    "CREATE (n8:Treatment {name:'Antiviral Medications'})\n",
    "CREATE (n9:Treatment {name:'Oxygen Therapy'})\n",
    "CREATE (n10:Disease {name:'COVID-19'})\n",
    "CREATE (n11:Drug {name:'Convalescent Plasma'})\n",
    "CREATE (n12:Drug {name:'Remdesivir'})\n",
    "CREATE (n13:Drug {name:'Dexamethasone'})\n",
    "CREATE (n14:Drug {name:'Hydroxychloroquine'})\n",
    "CREATE (n15:Drug {name:'Ivermectin'})\n",
    "CREATE (n16:symptom {name:'Fatigue'})\n",
    "CREATE (n17:symptom {name:'Loss of Taste or Smell'})\n",
    "CREATE (n18:symptom {name:'Shortness of Breath'})\n",
    "CREATE (n19:symptom {name:'Muscle Aches'})\n",
    "CREATE (n20:symptom {name:'Sore Throat'})\n",
    "\n",
    "CREATE (n21:Treatment {name:'Ventilator Support'})\n",
    "CREATE (n22:Treatment {name:'Anticoagulants'})\n",
    "CREATE (n23:Treatment {name:'Steroids'})\n",
    "CREATE (n24:Treatment {name:'Monoclonal Antibodies'})\n",
    "CREATE (n25:Treatment {name:'Mechanical Ventilation'})\n",
    "\n",
    "RETURN  n1, n2, n3, n4, n5,n6,n7,n8,n9,n10,n11,n12,n13,n14,n15,n16,n17,n18,n19,n20,n21,n22,n23,n24,n25\n",
    "###################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
